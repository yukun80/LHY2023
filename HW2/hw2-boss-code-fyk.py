"""
éŸ³ç´ åˆ†ç±»ï¼ˆPhoneme Classificationï¼‰æ·±åº¦å­¦ä¹ é¡¹ç›®
==============================================

ã€é¡¹ç›®èƒŒæ™¯ã€‘
éŸ³ç´ æ˜¯è¯­éŸ³è¯†åˆ«çš„åŸºæœ¬å•ä½ï¼Œå°±åƒæ–‡å­—çš„å­—æ¯ä¸€æ ·ã€‚è¿™ä¸ªé¡¹ç›®çš„ç›®æ ‡æ˜¯ï¼š
- è¾“å…¥ï¼šéŸ³é¢‘çš„MFCCç‰¹å¾ï¼ˆ39ç»´å‘é‡ï¼‰
- è¾“å‡ºï¼šé¢„æµ‹è¯¥éŸ³é¢‘å¸§å±äºå“ªä¸ªéŸ³ç´ ï¼ˆ41ä¸ªéŸ³ç´ ç±»åˆ«ä¹‹ä¸€ï¼‰

ã€ä¸ºä»€ä¹ˆé‡è¦ã€‘
éŸ³ç´ åˆ†ç±»æ˜¯è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå‡†ç¡®çš„éŸ³ç´ è¯†åˆ«æ˜¯è¯­éŸ³è½¬æ–‡å­—çš„åŸºç¡€ã€‚

ã€æŠ€æœ¯äº®ç‚¹ã€‘
1. ä½¿ç”¨BiLSTMæ•è·éŸ³é¢‘çš„æ—¶åºç‰¹æ€§
2. åºåˆ—çº§å¤„ç†ï¼Œè€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯
3. å®Œå–„çš„é˜²è¿‡æ‹Ÿåˆç­–ç•¥
4. é«˜æ•ˆçš„å˜é•¿åºåˆ—å¤„ç†

ä½œè€…ï¼šæ·±åº¦å­¦ä¹ åˆå­¦è€…å‹å¥½ç‰ˆ
"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
import random
import os
from tqdm import tqdm


def same_seeds(seed):
    """
    è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒç»“æœå¯é‡å¤

    ã€ä¸ºä»€ä¹ˆéœ€è¦ã€‘
    æ·±åº¦å­¦ä¹ ä¸­æœ‰å¾ˆå¤šéšæœºè¿‡ç¨‹ï¼ˆæƒé‡åˆå§‹åŒ–ã€æ•°æ®shuffleã€dropoutç­‰ï¼‰ï¼Œ
    è®¾ç½®å›ºå®šç§å­å¯ä»¥è®©æ¯æ¬¡è¿è¡Œå¾—åˆ°ç›¸åŒç»“æœï¼Œä¾¿äºè°ƒè¯•å’Œæ¯”è¾ƒã€‚

    ã€å‚æ•°ã€‘
    seed (int): éšæœºç§å­æ•°å€¼

    ã€æ¶‰åŠçš„éšæœºæ€§ã€‘
    - NumPyéšæœºæ•°ç”Ÿæˆ
    - Pythonå†…ç½®éšæœºæ•°
    - PyTorch CPUéšæœºæ•°
    - PyTorch GPUéšæœºæ•°
    - CUDNNåç«¯çš„éšæœºæ€§
    """
    random.seed(seed)  # Pythonå†…ç½®randomæ¨¡å—
    np.random.seed(seed)  # NumPyéšæœºæ•°
    torch.manual_seed(seed)  # PyTorch CPUéšæœºæ•°
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)  # å½“å‰GPUéšæœºæ•°
        torch.cuda.manual_seed_all(seed)  # æ‰€æœ‰GPUéšæœºæ•°
    # å…³é—­CUDNNçš„ä¼˜åŒ–ï¼Œç¡®ä¿ç»“æœå®Œå…¨ä¸€è‡´ï¼ˆä½†ä¼šç¨å¾®æ…¢ä¸€äº›ï¼‰
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def load_feat(path):
    """
    åŠ è½½é¢„å¤„ç†çš„MFCCç‰¹å¾æ–‡ä»¶

    ã€MFCCç‰¹å¾è¯´æ˜ã€‘
    MFCC (Mel-Frequency Cepstral Coefficients) æ˜¯éŸ³é¢‘ä¿¡å·å¤„ç†ä¸­çš„ç»å…¸ç‰¹å¾ï¼š
    - æ¨¡æ‹Ÿäººè€³å¯¹å£°éŸ³çš„æ„ŸçŸ¥ç‰¹æ€§
    - ç»´åº¦é€šå¸¸æ˜¯39ç»´ï¼ˆ13ä¸ªé™æ€ + 13ä¸ªä¸€é˜¶å·®åˆ† + 13ä¸ªäºŒé˜¶å·®åˆ†ï¼‰
    - æ¯”åŸå§‹éŸ³é¢‘æ³¢å½¢æ›´é€‚åˆæœºå™¨å­¦ä¹ æ¨¡å‹å¤„ç†

    ã€å‚æ•°ã€‘
    path (str): ç‰¹å¾æ–‡ä»¶è·¯å¾„ï¼Œé€šå¸¸æ˜¯.ptæ ¼å¼ï¼ˆPyTorchå¼ é‡ï¼‰

    ã€è¿”å›ã€‘
    torch.Tensor: åŠ è½½çš„ç‰¹å¾å¼ é‡
    """
    feat = torch.load(path)
    return feat


# ä»¥ä¸‹è¢«æ³¨é‡Šçš„ä»£ç æ˜¯ä¼ ç»Ÿçš„ç‰¹å¾æ‹¼æ¥æ–¹æ³•ï¼Œç°åœ¨æˆ‘ä»¬ç”¨æ›´å…ˆè¿›çš„åºåˆ—æ¨¡å‹æ›¿ä»£
# def shift(x, n):
#     """
#     æ—¶é—´å¸§ç§»ä½è¾…åŠ©å‡½æ•°
#
#     ã€ä¼ ç»Ÿæ–¹æ³•ã€‘
#     åœ¨RNNæ™®åŠä¹‹å‰ï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯æ‰‹åŠ¨æ‹¼æ¥ç›¸é‚»å¸§çš„ç‰¹å¾ï¼Œ
#     ä¸ºæ¨¡å‹æä¾›æ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼š
#     - å½“å‰å¸§ + å‰2å¸§ + å2å¸§ = 5å¸§æ‹¼æ¥
#     - ç‰¹å¾ç»´åº¦ä»39ç»´å˜æˆ195ç»´ (39 Ã— 5)
#
#     ã€ä¸ºä»€ä¹ˆç°åœ¨ä¸ç”¨ã€‘
#     BiLSTMèƒ½è‡ªåŠ¨å­¦ä¹ æ—¶åºä¾èµ–å…³ç³»ï¼Œæ¯”æ‰‹å·¥æ‹¼æ¥æ›´å¼ºå¤§
#     """


# def concat_feat(x, concat_n):
#     """
#     è¿æ¥ç›¸é‚»å¸§ç‰¹å¾çš„ä¼ ç»Ÿæ–¹æ³•
#
#     ã€åŸç†ã€‘
#     éŸ³ç´ è¯†åˆ«éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡ï¼Œå› ä¸ºï¼š
#     - åŒä¸€ä¸ªéŸ³ç´ åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å£°å­¦ç‰¹æ€§ä¼šå˜åŒ–
#     - ç›¸é‚»éŸ³ç´ ä¹‹é—´å­˜åœ¨åè°ƒå‘éŸ³æ•ˆåº”
#
#     ã€ç°ä»£æ›¿ä»£æ–¹æ¡ˆã€‘
#     RNN/LSTM/Transformerç­‰åºåˆ—æ¨¡å‹èƒ½æ›´å¥½åœ°å»ºæ¨¡æ—¶åºå…³ç³»
#     """


# class LibriDataset(Dataset):
#     """
#     ä¼ ç»Ÿçš„æ•°æ®é›†å®ç°ï¼ˆé€å¸§å¤„ç†ï¼‰
#
#     ã€é—®é¢˜ã€‘
#     - å°†åºåˆ—æ‰“æ•£æˆå•ç‹¬çš„å¸§ï¼Œä¸¢å¤±äº†åºåˆ—ç»“æ„
#     - æ— æ³•å……åˆ†åˆ©ç”¨RNNçš„åºåˆ—å»ºæ¨¡èƒ½åŠ›
#     - å†…å­˜ä½¿ç”¨æ•ˆç‡ä½
#     """


class LibriDataset(Dataset):
    """
    æ”¹è¿›çš„åºåˆ—çº§æ•°æ®é›†ç±»

    ã€è®¾è®¡ç†å¿µã€‘
    ä»¥å®Œæ•´çš„å¥å­ï¼ˆutteranceï¼‰ä¸ºå•ä½è¿›è¡Œå¤„ç†ï¼Œè¿™æ ·ï¼š
    1. ä¿æŒäº†éŸ³é¢‘çš„è‡ªç„¶åºåˆ—ç»“æ„
    2. è®©RNNæ¨¡å‹èƒ½å¤Ÿå……åˆ†å‘æŒ¥åºåˆ—å»ºæ¨¡èƒ½åŠ›
    3. å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆï¼ˆæŒ‰éœ€åŠ è½½ï¼‰
    4. æ›´ç¬¦åˆå®é™…åº”ç”¨åœºæ™¯

    ã€æ•°æ®ç»„ç»‡ã€‘
    - è®­ç»ƒé›†ï¼šç”¨äºå­¦ä¹ æ¨¡å‹å‚æ•°
    - éªŒè¯é›†ï¼šç”¨äºè°ƒæ•´è¶…å‚æ•°å’Œæ—©åœ
    - æµ‹è¯•é›†ï¼šç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°
    """

    def __init__(self, split, feat_dir, phone_path, train_ratio=0.8):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        ã€å‚æ•°è¯´æ˜ã€‘
        split (str): æ•°æ®é›†åˆ’åˆ† - "train"ï¼ˆè®­ç»ƒï¼‰, "val"ï¼ˆéªŒè¯ï¼‰, "test"ï¼ˆæµ‹è¯•ï¼‰
        feat_dir (str): ç‰¹å¾æ–‡ä»¶ç›®å½•è·¯å¾„
        phone_path (str): éŸ³ç´ æ ‡ç­¾æ–‡ä»¶ç›®å½•è·¯å¾„
        train_ratio (float): è®­ç»ƒé›†å æ¯”ï¼Œå‰©ä½™éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†

        ã€æ•°æ®é›†åˆ’åˆ†åŸç†ã€‘
        - è®­ç»ƒé›†ï¼šç”¨äºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ¨¡å‹å‚æ•°
        - éªŒè¯é›†ï¼šç”¨äºè¶…å‚æ•°è°ƒä¼˜å’Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        - æµ‹è¯•é›†ï¼šæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼Œä¸èƒ½ç”¨äºè°ƒå‚
        """
        self.split = split
        self.feat_dir = feat_dir

        # æ ¹æ®splitç¡®å®šå·¥ä½œæ¨¡å¼å’Œæ–‡ä»¶è·¯å¾„
        if split == "train" or split == "val":
            mode = "train"  # è®­ç»ƒå’ŒéªŒè¯éƒ½ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æ–‡ä»¶
            usage_list_path = os.path.join(phone_path, "train_split.txt")
            label_path = os.path.join(phone_path, f"{mode}_labels.txt")
        else:  # split == "test"
            mode = "test"
            usage_list_path = os.path.join(phone_path, "test_split.txt")
            label_path = None  # æµ‹è¯•é›†æ²¡æœ‰æ ‡ç­¾ï¼ˆè¿™æ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„ï¼‰

        # åŠ è½½æ–‡ä»¶ååˆ—è¡¨
        with open(usage_list_path) as f:
            usage_list = f.read().splitlines()

        # å¦‚æœæ˜¯è®­ç»ƒæˆ–éªŒè¯æ¨¡å¼ï¼Œéœ€è¦åˆ’åˆ†æ•°æ®é›†
        if split == "train" or split == "val":
            train_len = int(len(usage_list) * train_ratio)
            if split == "train":
                self.usage_list = usage_list[:train_len]  # å‰80%ä½œä¸ºè®­ç»ƒé›†
            else:  # split == "val"
                self.usage_list = usage_list[train_len:]  # å20%ä½œä¸ºéªŒè¯é›†
        else:  # split == "test"
            self.usage_list = usage_list

        print(f"[Dataset] - {split}é›†å¥å­æ•°é‡: {len(self.usage_list)}")

        # åŠ è½½æ ‡ç­¾å­—å…¸ï¼ˆä»…åœ¨æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼‰
        self.label_dict = {}
        if label_path:
            with open(label_path) as f:
                for line in f.read().splitlines():
                    parts = line.strip().split()
                    # parts[0]æ˜¯æ–‡ä»¶åï¼Œparts[1:]æ˜¯è¯¥å¥å­æ¯ä¸€å¸§çš„éŸ³ç´ æ ‡ç­¾
                    self.label_dict[parts[0]] = [int(p) for p in parts[1:]]

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°ï¼ˆå¥å­æ•°é‡ï¼‰"""
        return len(self.usage_list)

    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ•°æ®æ ·æœ¬

        ã€è¿”å›æ ¼å¼ã€‘
        - è®­ç»ƒ/éªŒè¯æ¨¡å¼ï¼š(features, labels)
          - features: (seq_len, 39) ä¸€ä¸ªå¥å­çš„MFCCç‰¹å¾åºåˆ—
          - labels: (seq_len,) å¯¹åº”æ¯ä¸€å¸§çš„éŸ³ç´ æ ‡ç­¾
        - æµ‹è¯•æ¨¡å¼ï¼šfeatures
          - features: (seq_len, 39) å¾…é¢„æµ‹çš„ç‰¹å¾åºåˆ—

        ã€ä¸ºä»€ä¹ˆè¿”å›æ•´ä¸ªåºåˆ—ã€‘
        RNNæ¨¡å‹éœ€è¦å®Œæ•´çš„åºåˆ—ä¿¡æ¯æ¥å»ºæ¨¡æ—¶åºä¾èµ–å…³ç³»
        """
        fname = self.usage_list[idx]
        # æ„é€ ç‰¹å¾æ–‡ä»¶è·¯å¾„
        feat_path = os.path.join(self.feat_dir, "train" if self.split != "test" else "test", f"{fname}.pt")

        # åŠ è½½å•ä¸ªå¥å­çš„MFCCç‰¹å¾
        features = torch.load(feat_path)  # Shape: (seq_len, 39)

        if self.split == "test":
            return features  # æµ‹è¯•é›†åªè¿”å›ç‰¹å¾ï¼Œæ²¡æœ‰æ ‡ç­¾
        else:
            # è®­ç»ƒå’ŒéªŒè¯é›†éœ€è¦è¿”å›ç‰¹å¾å’Œå¯¹åº”çš„æ ‡ç­¾
            labels = torch.LongTensor(self.label_dict[fname])  # Shape: (seq_len,)
            return features, labels


def collate_fn(batch):
    """
    è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œå¤„ç†å˜é•¿åºåˆ—

    ã€é—®é¢˜èƒŒæ™¯ã€‘
    ä¸åŒå¥å­çš„é•¿åº¦ä¸åŒï¼Œä½†PyTorchçš„DataLoaderéœ€è¦å°†å¤šä¸ªæ ·æœ¬ç»„æˆå›ºå®šå¤§å°çš„batchã€‚
    è¿™å°±éœ€è¦å¯¹çŸ­åºåˆ—è¿›è¡Œå¡«å……ï¼ˆpaddingï¼‰ï¼Œä½¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´ã€‚

    ã€è§£å†³æ–¹æ¡ˆã€‘
    1. æ‰¾åˆ°batchä¸­æœ€é•¿çš„åºåˆ—
    2. ç”¨ç‰¹æ®Šå€¼å¡«å……çŸ­åºåˆ—åˆ°ç›¸åŒé•¿åº¦
    3. è®°å½•æ¯ä¸ªåºåˆ—çš„åŸå§‹é•¿åº¦ï¼Œé¿å…æ¨¡å‹å¤„ç†å¡«å……éƒ¨åˆ†

    ã€å‚æ•°ã€‘
    batch (list): DataLoaderä¼ å…¥çš„ä¸€ä¸ªbatchçš„æ•°æ®

    ã€è¿”å›ã€‘
    æ ¹æ®æ¨¡å¼ä¸åŒï¼š
    - è®­ç»ƒ/éªŒè¯: (padded_features, padded_labels, lengths)
    - æµ‹è¯•: (padded_features, lengths)
    """
    # åˆ¤æ–­æ˜¯è®­ç»ƒ/éªŒè¯æ¨¡å¼è¿˜æ˜¯æµ‹è¯•æ¨¡å¼
    if isinstance(batch[0], tuple):
        # è®­ç»ƒå’ŒéªŒè¯æ¨¡å¼ï¼šæ¯ä¸ªå…ƒç´ æ˜¯(features, labels)å…ƒç»„
        features, labels = zip(*batch)

        # è®°å½•æ¯ä¸ªåºåˆ—çš„åŸå§‹é•¿åº¦ï¼Œè¿™å¾ˆé‡è¦ï¼
        lengths = torch.LongTensor([len(f) for f in features])

        # ä½¿ç”¨PyTorchæä¾›çš„pad_sequenceå‡½æ•°è¿›è¡Œå¡«å……
        # batch_first=True è¡¨ç¤ºè¾“å‡ºç»´åº¦æ˜¯ (batch_size, seq_len, feature_dim)
        padded_features = pad_sequence(features, batch_first=True)

        # æ ‡ç­¾å¡«å……ä½¿ç”¨-1ï¼Œè¿™æ ·åœ¨è®¡ç®—æŸå¤±æ—¶å¯ä»¥å¿½ç•¥å¡«å……ä½ç½®
        padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1)

        return padded_features, padded_labels, lengths
    else:
        # æµ‹è¯•æ¨¡å¼ï¼šæ¯ä¸ªå…ƒç´ åªæœ‰features
        features = batch
        lengths = torch.LongTensor([len(f) for f in features])
        padded_features = pad_sequence(features, batch_first=True)
        return padded_features, lengths


class SequentialBossModel(nn.Module):
    """
    åŸºäºBiLSTMçš„åºåˆ—éŸ³ç´ åˆ†ç±»æ¨¡å‹

    ã€æ¨¡å‹è®¾è®¡æ€æƒ³ã€‘
    1. é¢„å¤„ç†ç½‘ç»œï¼šå°†åŸå§‹ç‰¹å¾æ˜ å°„åˆ°åˆé€‚çš„è¡¨ç¤ºç©ºé—´
    2. BiLSTMç½‘ç»œï¼šæ•è·åŒå‘çš„æ—¶åºä¾èµ–å…³ç³»
    3. åˆ†ç±»å™¨ï¼šå°†åºåˆ—è¡¨ç¤ºæ˜ å°„åˆ°éŸ³ç´ ç±»åˆ«

    ã€ä¸ºä»€ä¹ˆå«"Boss"æ¨¡å‹ã€‘
    è®¾è®¡ç›®æ ‡æ˜¯è¶…è¶Šä¼ ç»Ÿçš„ç®€å•åŸºå‡†æ¨¡å‹ï¼ˆBoss Baselineï¼‰ï¼Œ
    é€šè¿‡åºåˆ—å»ºæ¨¡æŠ€æœ¯æ˜¾è‘—æå‡éŸ³ç´ åˆ†ç±»æ€§èƒ½ã€‚

    ã€BiLSTMçš„ä¼˜åŠ¿ã€‘
    - åŒå‘å¤„ç†ï¼šåŒæ—¶è€ƒè™‘è¿‡å»å’Œæœªæ¥çš„ä¿¡æ¯
    - é•¿æœŸè®°å¿†ï¼šèƒ½å¤Ÿæ•è·é•¿è·ç¦»çš„æ—¶åºä¾èµ–
    - è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ï¼šä¸éœ€è¦æ‰‹å·¥è®¾è®¡æ—¶åºç‰¹å¾
    """

    def __init__(self, input_dim=39, output_dim=41, hidden_dim=256, num_layers=3, dropout=0.3):
        """
        åˆå§‹åŒ–æ¨¡å‹ç»“æ„

        ã€å‚æ•°è¯´æ˜ã€‘
        input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ŒMFCCæ˜¯39ç»´
        output_dim (int): è¾“å‡ºç±»åˆ«æ•°ï¼Œè‹±è¯­éŸ³ç´ æœ‰41ä¸ª
        hidden_dim (int): LSTMéšè—å±‚ç»´åº¦ï¼Œæ§åˆ¶æ¨¡å‹å®¹é‡
        num_layers (int): LSTMå±‚æ•°ï¼Œæ›´æ·±çš„ç½‘ç»œå­¦ä¹ èƒ½åŠ›æ›´å¼º
        dropout (float): Dropoutæ¯”ä¾‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

        ã€ç½‘ç»œç»“æ„è®¾è®¡åŸåˆ™ã€‘
        - é¢„å¤„ç†ç½‘ç»œï¼šæå‡ç‰¹å¾è¡¨ç¤ºè´¨é‡
        - å¤šå±‚BiLSTMï¼šé€å±‚æŠ½è±¡æ—¶åºæ¨¡å¼
        - é€‚å½“çš„æ­£åˆ™åŒ–ï¼šä¿è¯æ³›åŒ–èƒ½åŠ›
        """
        super().__init__()

        # é¢„å¤„ç†ç½‘ç»œï¼šå°†39ç»´MFCCç‰¹å¾æ˜ å°„åˆ°æ¨¡å‹çš„éšè—ç»´åº¦
        # ä¸ºä»€ä¹ˆéœ€è¦é¢„å¤„ç†ï¼š
        # 1. ç‰¹å¾å½’ä¸€åŒ–å’Œéçº¿æ€§å˜æ¢
        # 2. ç»´åº¦é€‚é…ï¼Œä¸ºåç»­LSTMå‡†å¤‡
        # 3. æä¾›åˆæ­¥çš„ç‰¹å¾æŠ½è±¡
        self.prenet = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),  # çº¿æ€§å˜æ¢
            nn.LayerNorm(hidden_dim),  # å±‚å½’ä¸€åŒ–ï¼Œç¨³å®šè®­ç»ƒ
            nn.ReLU(),  # éçº¿æ€§æ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),  # éšæœºå¤±æ´»ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        )

        # BiLSTMç½‘ç»œï¼šæ ¸å¿ƒçš„åºåˆ—å»ºæ¨¡ç»„ä»¶
        # ä¸ºä»€ä¹ˆé€‰æ‹©BiLSTMï¼š
        # 1. åŒå‘ï¼šéŸ³ç´ è¯†åˆ«éœ€è¦è€ƒè™‘å‰åæ–‡
        # 2. LSTMï¼šæ¯”æ™®é€šRNNæ›´å¥½åœ°å¤„ç†é•¿åºåˆ—
        # 3. å¤šå±‚ï¼šæä¾›åˆ†å±‚çš„ç‰¹å¾æŠ½è±¡
        self.bilstm = nn.LSTM(
            input_size=hidden_dim,  # è¾“å…¥ç»´åº¦
            hidden_size=hidden_dim,  # éšè—çŠ¶æ€ç»´åº¦
            num_layers=num_layers,  # å±‚æ•°
            bidirectional=True,  # åŒå‘å¤„ç†
            batch_first=True,  # è¾“å…¥æ ¼å¼ (batch, seq, feature)
            dropout=dropout if num_layers > 1 else 0,  # å±‚é—´dropout
        )

        # åˆ†ç±»å™¨ï¼šå°†BiLSTMçš„è¾“å‡ºæ˜ å°„åˆ°éŸ³ç´ ç±»åˆ«
        # æ³¨æ„ï¼šBiLSTMè¾“å‡ºç»´åº¦æ˜¯ hidden_dim * 2ï¼ˆåŒå‘æ‹¼æ¥ï¼‰
        self.classifier = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x, lengths):
        """
        å‰å‘ä¼ æ’­è¿‡ç¨‹

        ã€å‚æ•°ã€‘
        x: (batch_size, max_seq_len, feature_dim) å¡«å……åçš„ç‰¹å¾åºåˆ—
        lengths: (batch_size,) æ¯ä¸ªåºåˆ—çš„åŸå§‹é•¿åº¦

        ã€è¿”å›ã€‘
        logits: (batch_size, max_seq_len, output_dim) æ¯å¸§çš„ç±»åˆ«é¢„æµ‹åˆ†æ•°

        ã€å¤„ç†æµç¨‹è¯¦è§£ã€‘
        """
        # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†ç½‘ç»œ
        # å°†39ç»´MFCCç‰¹å¾æ˜ å°„åˆ°hidden_dimç»´ç©ºé—´
        x = self.prenet(x)  # -> (batch_size, max_seq_len, hidden_dim)

        # ç¬¬äºŒæ­¥ï¼šæ‰“åŒ…å˜é•¿åºåˆ—
        # è¿™æ˜¯å¤„ç†å¡«å……åºåˆ—çš„æ ‡å‡†åšæ³•ï¼Œå¯ä»¥ï¼š
        # 1. é¿å…æ¨¡å‹å¤„ç†å¡«å……éƒ¨åˆ†
        # 2. æé«˜è®¡ç®—æ•ˆç‡
        # 3. å¾—åˆ°æ­£ç¡®çš„éšè—çŠ¶æ€
        packed_x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)

        # ç¬¬ä¸‰æ­¥ï¼šé€šè¿‡BiLSTMå¤„ç†åºåˆ—
        # BiLSTMä¼šè‡ªåŠ¨å­¦ä¹ æ—¶åºä¾èµ–å…³ç³»
        packed_out, _ = self.bilstm(packed_x)

        # ç¬¬å››æ­¥ï¼šè§£åŒ…åºåˆ—ï¼Œè¿˜åŸä¸ºå¡«å……æ ¼å¼
        # è¿™æ ·åç»­å¤„ç†æ›´æ–¹ä¾¿
        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)
        # lstm_out shape: (batch_size, max_seq_len, hidden_dim * 2)

        # ç¬¬äº”æ­¥ï¼šåˆ†ç±»å™¨è¿›è¡Œé€å¸§é¢„æµ‹
        # æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½è¦é¢„æµ‹å¯¹åº”çš„éŸ³ç´ ç±»åˆ«
        logits = self.classifier(lstm_out)
        # logits shape: (batch_size, max_seq_len, output_dim=41)

        return logits


class AntiOverfittingTrainer:
    """
    ä¸“é—¨è®¾è®¡ç”¨äºå¯¹æŠ—è¿‡æ‹Ÿåˆçš„è®­ç»ƒå™¨

    ã€è¿‡æ‹Ÿåˆé—®é¢˜ã€‘
    æ·±åº¦å­¦ä¹ æ¨¡å‹å®¹æ˜“åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°å·®ã€‚
    è¿™ç§ç°è±¡å«è¿‡æ‹Ÿåˆï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€ã€‚

    ã€å¯¹æŠ—ç­–ç•¥ã€‘
    1. æ­£åˆ™åŒ–æŠ€æœ¯ï¼šæƒé‡è¡°å‡ã€Dropoutã€æ ‡ç­¾å¹³æ»‘
    2. å­¦ä¹ ç‡è°ƒåº¦ï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
    3. æ—©åœæœºåˆ¶ï¼šé˜²æ­¢è¿‡åº¦è®­ç»ƒ
    4. æ¢¯åº¦è£å‰ªï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹

    ã€è®­ç»ƒç›‘æ§ã€‘
    åŒæ—¶è·Ÿè¸ªè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ€§èƒ½ï¼ŒåŠæ—¶å‘ç°è¿‡æ‹Ÿåˆ
    """

    def __init__(self, model, device, num_epochs=50, patience=10):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        ã€å‚æ•°ã€‘
        model: è¦è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¨¡å‹
        device: è®¡ç®—è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        num_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
        patience: æ—©åœè€å¿ƒå€¼ï¼ŒéªŒè¯æ€§èƒ½å¤šå°‘è½®ä¸æå‡å°±åœæ­¢è®­ç»ƒ
        """
        self.model = model
        self.device = device
        self.num_epochs = num_epochs
        self.best_val_acc = 0.0  # è®°å½•æœ€ä½³éªŒè¯å‡†ç¡®ç‡
        self.patience = patience
        self.patience_counter = 0  # æ—©åœè®¡æ•°å™¨

        # è®­ç»ƒå†å²è®°å½•ï¼Œç”¨äºåˆ†æè®­ç»ƒè¿‡ç¨‹
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []

    def create_optimizer_and_scheduler(self, learning_rate=1e-4, weight_decay=1e-5):
        """
        åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨

        ã€ä¼˜åŒ–å™¨é€‰æ‹©ï¼šAdamWã€‘
        - Adamï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œè®­ç»ƒç¨³å®š
        - Wï¼ˆæƒé‡è¡°å‡ï¼‰ï¼šL2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

        ã€å­¦ä¹ ç‡è°ƒåº¦ï¼šReduceLROnPlateauã€‘
        å½“éªŒè¯å‡†ç¡®ç‡åœæ­¢æå‡æ—¶ï¼Œè‡ªåŠ¨é™ä½å­¦ä¹ ç‡
        è¿™æ˜¯ä¸€ç§å¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

        ã€æŸå¤±å‡½æ•°ï¼šCrossEntropyLossã€‘
        - æ ‡å‡†çš„å¤šåˆ†ç±»æŸå¤±å‡½æ•°
        - æ ‡ç­¾å¹³æ»‘ï¼šå‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
        - ignore_index=-1ï¼šå¿½ç•¥å¡«å……ä½ç½®çš„æŸå¤±è®¡ç®—
        """
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,  # L2æ­£åˆ™åŒ–å¼ºåº¦
        )

        # åŸºäºéªŒè¯å‡†ç¡®ç‡çš„å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            "max",  # ç›‘æ§éªŒè¯å‡†ç¡®ç‡ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
            factor=0.5,  # å­¦ä¹ ç‡è¡°å‡å› å­
            patience=3,  # 3è½®ä¸æå‡å°±é™ä½å­¦ä¹ ç‡
        )

        # äº¤å‰ç†µæŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(
            label_smoothing=0.1, ignore_index=-1  # æ ‡ç­¾å¹³æ»‘ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ  # å¿½ç•¥å¡«å……ä½ç½®ï¼ˆæ ‡ç­¾ä¸º-1ï¼‰
        )

    def train_epoch(self, train_loader):
        """
        è®­ç»ƒä¸€ä¸ªepochï¼ˆä¸€è½®å®Œæ•´çš„è®­ç»ƒæ•°æ®éå†ï¼‰

        ã€è®­ç»ƒæ­¥éª¤ã€‘
        1. è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        2. éå†æ‰€æœ‰æ‰¹æ¬¡æ•°æ®
        3. å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±
        4. åå‘ä¼ æ’­æ›´æ–°å‚æ•°
        5. è®°å½•è®­ç»ƒæŒ‡æ ‡

        ã€è¿”å›ã€‘
        (å¹³å‡æŸå¤±, å¹³å‡å‡†ç¡®ç‡)
        """
        self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œå¯ç”¨Dropoutå’ŒBatchNorm
        total_loss, total_correct, total_frames = 0.0, 0, 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
        for features, labels, lengths in tqdm(train_loader, desc="è®­ç»ƒä¸­"):
            # å°†æ•°æ®ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            features, labels = features.to(self.device), labels.to(self.device)

            # æ¸…é›¶æ¢¯åº¦ï¼ˆPyTorchéœ€è¦æ‰‹åŠ¨æ¸…é›¶ï¼‰
            self.optimizer.zero_grad()

            # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ¨¡å‹è¾“å‡º
            outputs = self.model(features, lengths)

            # è®¡ç®—æŸå¤±
            # éœ€è¦reshapeï¼šoutputs (B,T,C) -> (B*T,C), labels (B,T) -> (B*T)
            # è¿™æ˜¯å› ä¸ºCrossEntropyLossæœŸæœ›2Dè¾“å…¥
            loss = self.criterion(outputs.view(-1, 41), labels.view(-1))

            # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            # æ›´æ–°å‚æ•°
            self.optimizer.step()

            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåªè€ƒè™‘éå¡«å……éƒ¨åˆ†ï¼‰
            predicted = torch.argmax(outputs, dim=-1)  # è·å–é¢„æµ‹ç±»åˆ«
            correct_predictions = ((predicted == labels) & (labels != -1)).sum().item()
            num_frames = (labels != -1).sum().item()  # éå¡«å……å¸§æ•°

            # ç´¯ç§¯ç»Ÿè®¡ä¿¡æ¯
            total_loss += loss.item()
            total_correct += correct_predictions
            total_frames += num_frames

        # è¿”å›å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        return total_loss / len(train_loader), total_correct / total_frames

    def validate_epoch(self, val_loader):
        """
        éªŒè¯ä¸€ä¸ªepoch

        ã€éªŒè¯ç‰¹ç‚¹ã€‘
        1. ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ˆèŠ‚çœå†…å­˜å’Œè®¡ç®—ï¼‰
        2. æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­Dropoutï¼‰
        3. åªè®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œä¸æ›´æ–°å‚æ•°

        ã€ä½œç”¨ã€‘
        - ç›‘æ§æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„æ€§èƒ½
        - æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦çš„ä¾æ®
        - é˜²æ­¢è¿‡æ‹Ÿåˆçš„é‡è¦æ‰‹æ®µ
        """
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        total_loss, total_correct, total_frames = 0.0, 0, 0

        # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—
        with torch.no_grad():
            for features, labels, lengths in tqdm(val_loader, desc="éªŒè¯ä¸­"):
                features, labels = features.to(self.device), labels.to(self.device)

                outputs = self.model(features, lengths)
                loss = self.criterion(outputs.view(-1, 41), labels.view(-1))

                predicted = torch.argmax(outputs, dim=-1)
                correct_predictions = ((predicted == labels) & (labels != -1)).sum().item()
                num_frames = (labels != -1).sum().item()

                total_loss += loss.item()
                total_correct += correct_predictions
                total_frames += num_frames

        return total_loss / len(val_loader), total_correct / total_frames

    def train(self, train_loader, val_loader, model_path="./HW2/bilstm_model.ckpt"):
        """
        å®Œæ•´çš„è®­ç»ƒæµç¨‹

        ã€è®­ç»ƒå¾ªç¯ã€‘
        1. æ¯ä¸ªepochè®­ç»ƒä¸€è½®
        2. éªŒè¯æ¨¡å‹æ€§èƒ½
        3. æ›´æ–°å­¦ä¹ ç‡
        4. ä¿å­˜æœ€ä½³æ¨¡å‹
        5. æ£€æŸ¥æ—©åœæ¡ä»¶

        ã€æ—©åœæœºåˆ¶ã€‘
        å¦‚æœéªŒè¯å‡†ç¡®ç‡åœ¨patienceè½®å†…æ²¡æœ‰æå‡ï¼Œå°±åœæ­¢è®­ç»ƒã€‚
        è¿™é˜²æ­¢äº†è¿‡æ‹Ÿåˆï¼Œä¹ŸèŠ‚çœäº†è®­ç»ƒæ—¶é—´ã€‚
        """
        print("ğŸš€ å¼€å§‹åºåˆ—æ¨¡å‹è®­ç»ƒ...")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        print("ğŸ¯ ç›®æ ‡ï¼šé€šè¿‡åºåˆ—å»ºæ¨¡çªç ´Boss Baseline")
        print("=" * 80)

        for epoch in range(self.num_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss, train_acc = self.train_epoch(train_loader)
            # éªŒè¯ä¸€ä¸ªepoch
            val_loss, val_acc = self.validate_epoch(val_loader)

            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆåŸºäºéªŒè¯å‡†ç¡®ç‡ï¼‰
            self.scheduler.step(val_acc)
            current_lr = self.optimizer.param_groups[0]["lr"]

            # è®°å½•è®­ç»ƒå†å²
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accs.append(train_acc)
            self.val_accs.append(val_acc)

            # è®¡ç®—è¿‡æ‹Ÿåˆå·®è·
            overfitting_gap = train_acc - val_acc

            # æ‰“å°è®­ç»ƒä¿¡æ¯
            print(f"ğŸ“Š Epoch [{epoch+1:03d}/{self.num_epochs:03d}]")
            print(f"   å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"   è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"   éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_acc:.4f}")
            print(f"   ğŸ” è¿‡æ‹Ÿåˆå·®è·: {overfitting_gap:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                torch.save(self.model.state_dict(), model_path)
                print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
                self.patience_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨
            else:
                self.patience_counter += 1
                print(f"   â³ ç­‰å¾…æ”¹å–„... ({self.patience_counter}/{self.patience})")

            print("-" * 80)

            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= self.patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break

        print("ğŸ è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.4f}")
        return self.best_val_acc


def main_improved_training(config, model_path):
    """
    åºåˆ—æ¨¡å‹çš„ä¸»è®­ç»ƒæµç¨‹

    ã€æµç¨‹æ¦‚è¿°ã€‘
    1. ç¯å¢ƒè®¾ç½®ï¼šéšæœºç§å­ã€è®¾å¤‡é€‰æ‹©
    2. æ•°æ®åŠ è½½ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†
    3. æ¨¡å‹åˆ›å»ºï¼šBiLSTMç½‘ç»œ
    4. è®­ç»ƒæ‰§è¡Œï¼šåè¿‡æ‹Ÿåˆè®­ç»ƒå™¨

    ã€å‚æ•°ã€‘
    config (dict): è¶…å‚æ•°é…ç½®å­—å…¸
    model_path (str): æ¨¡å‹ä¿å­˜è·¯å¾„
    """
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤
    same_seeds(config["seed"])

    # é€‰æ‹©è®¡ç®—è®¾å¤‡ï¼šGPUä¼˜å…ˆï¼ŒCPUå¤‡é€‰
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")

    # å®šä¹‰æ•°æ®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    feat_dir = r"E:\Document\code\LHY2023\HW2\libriphone\feat"
    phone_path = r"E:\Document\code\LHY2023\HW2\libriphone"

    # åˆ›å»ºæ•°æ®é›†å®ä¾‹
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    train_set = LibriDataset("train", feat_dir, phone_path, config["train_ratio"])
    val_set = LibriDataset("val", feat_dir, phone_path, config["train_ratio"])

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # DataLoaderè´Ÿè´£æ‰¹å¤„ç†ã€æ‰“ä¹±æ•°æ®ã€å¤šè¿›ç¨‹åŠ è½½ç­‰
    train_loader = DataLoader(
        train_set,
        batch_size=config["batch_size"],  # æ‰¹å¤§å°
        shuffle=True,  # æ‰“ä¹±æ•°æ®é¡ºåº
        num_workers=0,  # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
        collate_fn=collate_fn,  # è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
        pin_memory=True,  # åŠ é€ŸGPUä¼ è¾“
    )
    val_loader = DataLoader(
        val_set,
        batch_size=config["batch_size"],
        shuffle=False,  # éªŒè¯æ—¶ä¸éœ€è¦æ‰“ä¹±
        num_workers=0,
        collate_fn=collate_fn,
        pin_memory=True,
    )

    # åˆ›å»ºåºåˆ—æ¨¡å‹
    model = SequentialBossModel(
        input_dim=39,  # MFCCç‰¹å¾ç»´åº¦
        output_dim=41,  # éŸ³ç´ ç±»åˆ«æ•°
        hidden_dim=config["hidden_dim"],  # éšè—å±‚ç»´åº¦
        num_layers=config["num_layers"],  # LSTMå±‚æ•°
        dropout=config["dropout_rate"],  # Dropoutæ¯”ä¾‹
    ).to(
        device
    )  # ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡

    # åˆ›å»ºå¹¶é…ç½®è®­ç»ƒå™¨
    trainer = AntiOverfittingTrainer(model, device, num_epochs=config["num_epochs"], patience=config["patience"])
    trainer.create_optimizer_and_scheduler(learning_rate=config["learning_rate"], weight_decay=config["weight_decay"])

    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train(train_loader, val_loader, model_path=model_path)

    print("\nğŸŠ åºåˆ—æ¨¡å‹è®­ç»ƒå®Œæˆï¼")


def test_and_predict(model_path, config):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•å’Œé¢„æµ‹

    ã€é¢„æµ‹æµç¨‹ã€‘
    1. åŠ è½½æµ‹è¯•æ•°æ®
    2. é‡å»ºè®­ç»ƒå¥½çš„æ¨¡å‹
    3. é€æ‰¹é¢„æµ‹éŸ³ç´ ç±»åˆ«
    4. ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSVæ–‡ä»¶

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - æ¨¡å‹ç»“æ„å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    - éœ€è¦æ­£ç¡®å¤„ç†å˜é•¿åºåˆ—
    - é¢„æµ‹ç»“æœæ ¼å¼è¦ç¬¦åˆæäº¤è¦æ±‚

    ã€å‚æ•°ã€‘
    model_path (str): è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
    config (dict): æ¨¡å‹é…ç½®ï¼Œç¡®ä¿ç»“æ„ä¸€è‡´
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ”® å¼€å§‹æµ‹è¯•é¢„æµ‹ï¼Œè®¾å¤‡: {device}")

    # æ•°æ®è·¯å¾„ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    feat_dir = r"E:\Document\code\LHY2023\HW2\libriphone\feat"
    phone_path = r"E:\Document\code\LHY2023\HW2\libriphone"

    # åŠ è½½æµ‹è¯•æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
    test_set = LibriDataset("test", feat_dir, phone_path)
    test_loader = DataLoader(
        test_set, batch_size=config["batch_size"], shuffle=False, collate_fn=collate_fn  # æµ‹è¯•æ—¶ä¸æ‰“ä¹±é¡ºåº
    )
    print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆï¼æµ‹è¯•é›†å¥å­æ•°: {len(test_set):,}")

    # é‡å»ºæ¨¡å‹ç»“æ„ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
    model = SequentialBossModel(
        input_dim=39,
        output_dim=41,
        hidden_dim=config["hidden_dim"],
        num_layers=config["num_layers"],
        dropout=config["dropout_rate"],
    ).to(device)

    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°
    try:
        model.load_state_dict(torch.load(model_path))
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ '{model_path}'ã€‚è¯·ç¡®è®¤è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        return None

    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print(f"ğŸ“ å·²åŠ è½½æœ€ä½³æ¨¡å‹: {model_path}")

    # å¼€å§‹é¢„æµ‹
    print("ğŸ”® æ­£åœ¨ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    all_predictions = []

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for features, lengths in tqdm(test_loader, desc="é¢„æµ‹ä¸­"):
            features = features.to(device)

            # æ¨¡å‹å‰å‘ä¼ æ’­
            outputs = model(features, lengths)

            # è·å–é¢„æµ‹ç±»åˆ«ï¼ˆæ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼‰
            predicted_labels = torch.argmax(outputs, dim=-1)

            # æå–æ¯ä¸ªåºåˆ—çš„æœ‰æ•ˆé¢„æµ‹ï¼ˆæ’é™¤å¡«å……éƒ¨åˆ†ï¼‰
            for i in range(len(lengths)):
                valid_length = lengths[i]
                all_predictions.extend(predicted_labels[i, :valid_length].cpu().numpy())

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    final_predictions = np.array(all_predictions, dtype=np.int32)
    print("âœ… é¢„æµ‹å®Œæˆï¼")
    print(f"ğŸ“Š å…±é¢„æµ‹ {len(final_predictions):,} ä¸ªéŸ³ç´ å¸§")

    # ä¿å­˜é¢„æµ‹ç»“æœä¸ºCSVæ ¼å¼
    output_file = r"E:\Document\code\LHY2023\HW2\prediction_bilstm.csv"
    with open(output_file, "w") as f:
        f.write("Id,Class\n")  # CSVå¤´éƒ¨
        for i, y in enumerate(final_predictions):
            f.write(f"{i},{y}\n")

    print(f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    return final_predictions


if __name__ == "__main__":
    """
    ä¸»ç¨‹åºå…¥å£

    ã€è¶…å‚æ•°é…ç½®è¯´æ˜ã€‘
    - train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼Œå½±å“æ¨¡å‹çš„å­¦ä¹ æ•°æ®é‡
    - batch_size: æ‰¹å¤§å°ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§å’Œå†…å­˜ä½¿ç”¨
    - num_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
    - patience: æ—©åœè€å¿ƒå€¼ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    - learning_rate: å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿
    - weight_decay: æƒé‡è¡°å‡ï¼ŒL2æ­£åˆ™åŒ–å¼ºåº¦
    - seed: éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤
    - hidden_dim: éšè—å±‚ç»´åº¦ï¼Œæ§åˆ¶æ¨¡å‹å®¹é‡
    - num_layers: LSTMå±‚æ•°ï¼Œå½±å“æ¨¡å‹å¤æ‚åº¦
    - dropout_rate: Dropoutæ¯”ä¾‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

    ã€è¶…å‚æ•°è°ƒä¼˜å»ºè®®ã€‘
    1. å…ˆç”¨å°æ¨¡å‹å¿«é€ŸéªŒè¯æµç¨‹
    2. é€æ­¥å¢åŠ æ¨¡å‹å¤æ‚åº¦
    3. è§‚å¯Ÿè®­ç»ƒ/éªŒè¯æ›²çº¿ï¼Œè°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦
    4. ä½¿ç”¨éªŒè¯é›†æ€§èƒ½æŒ‡å¯¼è¶…å‚æ•°é€‰æ‹©
    """

    # è¶…å‚æ•°é…ç½®å­—å…¸
    config = {
        "train_ratio": 0.8,  # 80%æ•°æ®ç”¨äºè®­ç»ƒï¼Œ20%ç”¨äºéªŒè¯
        "batch_size": 32,  # æ‰¹å¤§å°ï¼Œå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒç¨³å®šæ€§
        "num_epochs": 100,  # æœ€å¤§è®­ç»ƒè½®æ•°
        "patience": 20,  # æ—©åœè€å¿ƒå€¼ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        "learning_rate": 5e-4,  # å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿
        "weight_decay": 1e-5,  # æƒé‡è¡°å‡ï¼ŒL2æ­£åˆ™åŒ–
        "seed": 3407,  # éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡å¤
        "hidden_dim": 512,  # LSTMéšè—ç»´åº¦ï¼Œæ§åˆ¶æ¨¡å‹å®¹é‡
        "num_layers": 4,  # LSTMå±‚æ•°ï¼Œæ›´æ·±çš„ç½‘ç»œå­¦ä¹ èƒ½åŠ›æ›´å¼º
        "dropout_rate": 0.4,  # Dropoutæ¯”ä¾‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    }

    # ç»Ÿä¸€çš„æ¨¡å‹ä¿å­˜è·¯å¾„
    MODEL_SAVE_PATH = "./bilstm_model_final.ckpt"

    print("ğŸ¯ é«˜çº§éŸ³ç´ åˆ†ç±»ï¼šåºåˆ—æ¨¡å‹è§£å†³æ–¹æ¡ˆ")
    print("=" * 60)

    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒ
        print("ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šå¼€å§‹è®­ç»ƒåºåˆ—æ¨¡å‹...")
        main_improved_training(config, MODEL_SAVE_PATH)

        # ç¬¬äºŒé˜¶æ®µï¼šæµ‹è¯•é¢„æµ‹
        print("\nğŸ”® ç¬¬äºŒé˜¶æ®µï¼šå¼€å§‹æµ‹è¯•é¢„æµ‹...")
        predictions = test_and_predict(MODEL_SAVE_PATH, config)
        if predictions is not None:
            print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è¯·æäº¤ prediction_bilstm.csv æ–‡ä»¶")
            print("\nğŸ“ˆ æ€§èƒ½æå‡å»ºè®®ï¼š")
            print("1. å°è¯•ä¸åŒçš„ç½‘ç»œç»“æ„ï¼ˆå±‚æ•°ã€ç»´åº¦ï¼‰")
            print("2. è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆdropoutã€weight_decayï¼‰")
            print("3. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥")
            print("4. è€ƒè™‘æ•°æ®å¢å¼ºæŠ€æœ¯")
            print("5. å°è¯•å…¶ä»–åºåˆ—æ¨¡å‹ï¼ˆTransformerï¼‰")

    except Exception as e:
        import traceback

        print(f"âŒ ä¸»æµç¨‹å‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()
        print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œé…ç½®")
        print("\nğŸ”§ å¸¸è§é—®é¢˜æ’æŸ¥ï¼š")
        print("1. æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®è®¤GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ")
        print("3. éªŒè¯PythonåŒ…ç‰ˆæœ¬å…¼å®¹æ€§")
        print("4. æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯è¿›è¡Œè°ƒè¯•")
